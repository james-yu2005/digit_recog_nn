{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34877,"sourceType":"datasetVersion","datasetId":27352}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndata = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-05T15:37:50.954865Z","iopub.execute_input":"2024-09-05T15:37:50.955508Z","iopub.status.idle":"2024-09-05T15:37:56.266647Z","shell.execute_reply.started":"2024-09-05T15:37:50.955449Z","shell.execute_reply":"2024-09-05T15:37:56.265175Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"data.head","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:37:56.269488Z","iopub.execute_input":"2024-09-05T15:37:56.269915Z","iopub.status.idle":"2024-09-05T15:37:56.290827Z","shell.execute_reply.started":"2024-09-05T15:37:56.269871Z","shell.execute_reply":"2024-09-05T15:37:56.289421Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"<bound method NDFrame.head of        label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n0          5    0    0    0    0    0    0    0    0    0  ...      0      0   \n1          0    0    0    0    0    0    0    0    0    0  ...      0      0   \n2          4    0    0    0    0    0    0    0    0    0  ...      0      0   \n3          1    0    0    0    0    0    0    0    0    0  ...      0      0   \n4          9    0    0    0    0    0    0    0    0    0  ...      0      0   \n...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n59995      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n59996      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n59997      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n59998      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n59999      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n\n       28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n0          0      0      0      0      0      0      0      0  \n1          0      0      0      0      0      0      0      0  \n2          0      0      0      0      0      0      0      0  \n3          0      0      0      0      0      0      0      0  \n4          0      0      0      0      0      0      0      0  \n...      ...    ...    ...    ...    ...    ...    ...    ...  \n59995      0      0      0      0      0      0      0      0  \n59996      0      0      0      0      0      0      0      0  \n59997      0      0      0      0      0      0      0      0  \n59998      0      0      0      0      0      0      0      0  \n59999      0      0      0      0      0      0      0      0  \n\n[60000 rows x 785 columns]>"},"metadata":{}}]},{"cell_type":"code","source":"# Turn data into a numpy array of data\ndata = np.array(data)\nm,n = data.shape\nprint(m,n)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:37:56.292509Z","iopub.execute_input":"2024-09-05T15:37:56.293543Z","iopub.status.idle":"2024-09-05T15:37:57.346008Z","shell.execute_reply.started":"2024-09-05T15:37:56.293477Z","shell.execute_reply":"2024-09-05T15:37:57.344587Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"60000 785\n","output_type":"stream"}]},{"cell_type":"markdown","source":"There are 60000 rows of training data and 785 cols because of (28 by 28) pixels + 1 label","metadata":{}},{"cell_type":"code","source":"# Shuffle data before training sets\nnp.random.shuffle(data)\n\n\ndata_dev = data[0:1000].T\n# First col of labels -> 1 * 60000\nY_dev = data_dev[0]\n# Rest of the rows and cols -> 784 * 60000\nX_dev = data_dev[1:n]\n# Normalize images because they are grascale (0 to 255)\nX_dev = X_dev / 255\n\n\ndata_train = data[1000:11000].T\nY_train = data_train[0]\nX_train = data_train[1:n]\nX_train = X_train/255\n\npixel_im,training_ex = X_train.shape\nprint(pixel_im,training_ex)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:37:57.349788Z","iopub.execute_input":"2024-09-05T15:37:57.350435Z","iopub.status.idle":"2024-09-05T15:37:59.546959Z","shell.execute_reply.started":"2024-09-05T15:37:57.350378Z","shell.execute_reply":"2024-09-05T15:37:59.545710Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"784 10000\n","output_type":"stream"}]},{"cell_type":"code","source":"def ReLU(Z):\n    return np.maximum(Z,0)\n\ndef init_params():\n    # Initialize random numpy arrays for the specific dimensions between\n    sub = 0.5\n    W1 = np.random.rand(10,784) - sub\n    b1 = np.random.rand(10,1) - sub\n    W2 = np.random.rand(10,10) - sub\n    b2 = np.random.rand(10,1) - sub\n    return W1,b1,W2,b2","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:37:59.548973Z","iopub.execute_input":"2024-09-05T15:37:59.549459Z","iopub.status.idle":"2024-09-05T15:37:59.557705Z","shell.execute_reply.started":"2024-09-05T15:37:59.549412Z","shell.execute_reply":"2024-09-05T15:37:59.556230Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def softmax(Z):\n    # Function to get max of 1 spread between final hidden layer to output layer\n    A = np.exp(Z) / sum(np.exp(Z))\n    return A\n\ndef ReLU_deriv(Z):\n    # Get derivative of ReLU which is one if its positive (straight line) or zero if its negative (flat line)\n    return Z > 0\n\n# one_hot takes in the label array of 1 * 60000\ndef one_hot(Y):\n    # Create numpy array with all zeroes of dimensions Y.size for rows and 10 for cols because 0-9 digits require 10 cols\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    \n    # Run through and set according values using arange and Y\n    # np.arange makes a np array from 0 to Y.size sequentially [0,1,2,3,4,etc...] and Y is the original label array [2,5,1,0,etc...]\n    # For each of these intersections it maps that exact 2D location to equal 1\n    # Looking at the two arrays, because 0 and 2 are the first nums of both then [0,2] = 1, then [1,5] = 1, and so on...\n    # This works out because the digits are from 0-9 so the label array, 'Y', accounts for the index starting from one ->\n    # For the first example, 2 will actually be inserted into the third row, while 5 will be inserted in the sixth, 1 into the second, and 0 into the first\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    \n    # Transpose it to match the rest of the data which has been transposed\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:37:59.559276Z","iopub.execute_input":"2024-09-05T15:37:59.559716Z","iopub.status.idle":"2024-09-05T15:37:59.576748Z","shell.execute_reply.started":"2024-09-05T15:37:59.559674Z","shell.execute_reply":"2024-09-05T15:37:59.574960Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"Input layer will contain the 784 units from the image, which are passed to a hidden layer that has 10 units using the ReLU activation (numbers from 0-1), which are finally output by the output layer with 10 units that represent the 0-9 digits using softmax so that the sum of the activations is 1 amongst all 0-9 digits, and we take the highest activation to be the most accurate digit chosen.\n\n**Forward propagation**\n\n$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n$$A^{[1]} = g_{\\text{ReLU}}(Z^{[1]}))$$\n$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n$$A^{[2]} = g_{\\text{softmax}}(Z^{[2]})$$","metadata":{}},{"cell_type":"code","source":"def forward_prop(W1, b1, W2, b2, X):\n    Z1 = W1.dot(X) + b1\n    A1 = ReLU(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:37:59.578614Z","iopub.execute_input":"2024-09-05T15:37:59.579145Z","iopub.status.idle":"2024-09-05T15:37:59.600512Z","shell.execute_reply.started":"2024-09-05T15:37:59.579081Z","shell.execute_reply":"2024-09-05T15:37:59.598640Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"**Backward propagation**\n\n$$dZ^{[2]} = A^{[2]} - Y$$\n$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n$$dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$$\n$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})$$\n$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}$$\n$$dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$$","metadata":{}},{"cell_type":"code","source":"def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n    m = Y.size\n    \n    # Creates one hot Y matrix\n    one_hot_Y = one_hot(Y)\n    \n    # Finds total difference between final output layer with softmax activation and actual Y\n    dZ2 = A2 - one_hot_Y\n    \n    # Average difference of (total difference between second layers times softmax activation layer)\n    dW2 = 1 / m * dZ2.dot(A1.T)\n    \n    # Finds average difference between second layers to determine how much bias should be\n    db2 = 1 / m * np.sum(dZ2)\n    \n    # Weights of second layer times difference between layers times ReLU derivative\n    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n    \n    # Average difference of (difference in first layer times original layer)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    \n    # Average differene of (sum of difference in first layer)\n    db1 = 1 / m * np.sum(dZ1)\n    return dW1, db1, dW2, db2","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:37:59.602284Z","iopub.execute_input":"2024-09-05T15:37:59.602755Z","iopub.status.idle":"2024-09-05T15:37:59.616658Z","shell.execute_reply.started":"2024-09-05T15:37:59.602711Z","shell.execute_reply":"2024-09-05T15:37:59.615048Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    # alpha is learning rate which we determine\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1    \n    W2 = W2 - alpha * dW2  \n    b2 = b2 - alpha * db2    \n    return W1, b1, W2, b2","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:37:59.618264Z","iopub.execute_input":"2024-09-05T15:37:59.619451Z","iopub.status.idle":"2024-09-05T15:37:59.640947Z","shell.execute_reply.started":"2024-09-05T15:37:59.619394Z","shell.execute_reply":"2024-09-05T15:37:59.638934Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def get_predictions(A2):\n    return np.argmax(A2, 0)\n\ndef get_accuracy(predictions, Y):\n    print(predictions, Y)\n    # Boolean comparison where if predictions is equal to Y, it adds one, then find average\n    return np.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, alpha, iterations):\n    W1, b1, W2, b2 = init_params()\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 10 == 0:\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A2)\n            print(get_accuracy(predictions, Y))\n    return W1, b1, W2, b2","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:37:59.645617Z","iopub.execute_input":"2024-09-05T15:37:59.646108Z","iopub.status.idle":"2024-09-05T15:37:59.660137Z","shell.execute_reply.started":"2024-09-05T15:37:59.646061Z","shell.execute_reply":"2024-09-05T15:37:59.658827Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.11, 1000)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:37:59.662233Z","iopub.execute_input":"2024-09-05T15:37:59.662722Z","iopub.status.idle":"2024-09-05T15:38:39.238759Z","shell.execute_reply.started":"2024-09-05T15:37:59.662660Z","shell.execute_reply":"2024-09-05T15:38:39.235768Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Iteration:  0\n[6 6 6 ... 6 0 0] [4 4 5 ... 8 0 0]\n0.1204\nIteration:  10\n[6 6 6 ... 6 0 0] [4 4 5 ... 8 0 0]\n0.1799\nIteration:  20\n[7 6 6 ... 6 0 0] [4 4 5 ... 8 0 0]\n0.2301\nIteration:  30\n[7 6 6 ... 6 0 0] [4 4 5 ... 8 0 0]\n0.2886\nIteration:  40\n[4 6 6 ... 6 0 0] [4 4 5 ... 8 0 0]\n0.3681\nIteration:  50\n[4 6 6 ... 3 0 0] [4 4 5 ... 8 0 0]\n0.4317\nIteration:  60\n[4 9 6 ... 3 0 0] [4 4 5 ... 8 0 0]\n0.4722\nIteration:  70\n[4 4 6 ... 3 0 0] [4 4 5 ... 8 0 0]\n0.5062\nIteration:  80\n[4 4 6 ... 6 0 0] [4 4 5 ... 8 0 0]\n0.5346\nIteration:  90\n[4 4 6 ... 6 0 0] [4 4 5 ... 8 0 0]\n0.5659\nIteration:  100\n[4 4 8 ... 6 0 0] [4 4 5 ... 8 0 0]\n0.5974\nIteration:  110\n[4 4 5 ... 6 0 0] [4 4 5 ... 8 0 0]\n0.6262\nIteration:  120\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.6519\nIteration:  130\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.6759\nIteration:  140\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.6963\nIteration:  150\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.7161\nIteration:  160\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.7307\nIteration:  170\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.7453\nIteration:  180\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.757\nIteration:  190\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.7691\nIteration:  200\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.7779\nIteration:  210\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.7871\nIteration:  220\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.7945\nIteration:  230\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.7999\nIteration:  240\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8046\nIteration:  250\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.808\nIteration:  260\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8131\nIteration:  270\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8162\nIteration:  280\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8206\nIteration:  290\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8238\nIteration:  300\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8275\nIteration:  310\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8316\nIteration:  320\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8343\nIteration:  330\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8361\nIteration:  340\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8389\nIteration:  350\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8418\nIteration:  360\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8436\nIteration:  370\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8462\nIteration:  380\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8482\nIteration:  390\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8507\nIteration:  400\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8526\nIteration:  410\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8534\nIteration:  420\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8549\nIteration:  430\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8559\nIteration:  440\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8577\nIteration:  450\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8591\nIteration:  460\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8597\nIteration:  470\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.861\nIteration:  480\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8616\nIteration:  490\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8623\nIteration:  500\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8632\nIteration:  510\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8643\nIteration:  520\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8652\nIteration:  530\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8664\nIteration:  540\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8677\nIteration:  550\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8687\nIteration:  560\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8696\nIteration:  570\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8698\nIteration:  580\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8706\nIteration:  590\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8715\nIteration:  600\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8715\nIteration:  610\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8721\nIteration:  620\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8728\nIteration:  630\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8736\nIteration:  640\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8744\nIteration:  650\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8749\nIteration:  660\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8755\nIteration:  670\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8766\nIteration:  680\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8776\nIteration:  690\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8778\nIteration:  700\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8785\nIteration:  710\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8791\nIteration:  720\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8798\nIteration:  730\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.88\nIteration:  740\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8803\nIteration:  750\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8806\nIteration:  760\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8807\nIteration:  770\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.881\nIteration:  780\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8814\nIteration:  790\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.882\nIteration:  800\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8826\nIteration:  810\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.883\nIteration:  820\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8834\nIteration:  830\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.884\nIteration:  840\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8845\nIteration:  850\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8847\nIteration:  860\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8849\nIteration:  870\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8852\nIteration:  880\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8855\nIteration:  890\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8857\nIteration:  900\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8863\nIteration:  910\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8869\nIteration:  920\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8871\nIteration:  930\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8875\nIteration:  940\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.888\nIteration:  950\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8881\nIteration:  960\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8883\nIteration:  970\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8888\nIteration:  980\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8893\nIteration:  990\n[4 4 5 ... 8 0 0] [4 4 5 ... 8 0 0]\n0.8894\n","output_type":"stream"}]},{"cell_type":"code","source":"def make_predictions(X, W1, b1, W2, b2):\n    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n    predictions = get_predictions(A2)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:39:55.803616Z","iopub.execute_input":"2024-09-05T15:39:55.804112Z","iopub.status.idle":"2024-09-05T15:39:55.813372Z","shell.execute_reply.started":"2024-09-05T15:39:55.804065Z","shell.execute_reply":"2024-09-05T15:39:55.811458Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\nget_accuracy(dev_predictions, Y_dev)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:39:57.444294Z","iopub.execute_input":"2024-09-05T15:39:57.444788Z","iopub.status.idle":"2024-09-05T15:39:57.487274Z","shell.execute_reply.started":"2024-09-05T15:39:57.444742Z","shell.execute_reply":"2024-09-05T15:39:57.484848Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"[9 6 4 9 1 7 9 9 5 3 9 2 9 9 5 2 4 4 1 1 1 7 4 2 6 4 1 2 6 3 5 2 5 9 8 2 3\n 9 1 2 9 6 4 6 5 4 7 0 6 5 2 0 6 6 9 2 8 4 2 9 3 6 9 2 4 5 3 9 7 9 6 1 8 4\n 3 6 4 7 5 4 6 3 1 3 3 2 2 2 6 8 8 4 2 5 9 7 4 9 2 8 6 7 2 1 4 7 7 2 5 6 8\n 0 1 8 9 0 1 3 3 5 4 0 8 5 8 8 5 0 0 7 6 9 8 9 7 1 5 2 5 5 0 0 9 9 3 0 9 9\n 3 0 7 8 6 1 6 9 7 0 6 0 3 3 6 3 4 8 2 6 7 0 0 0 4 2 0 9 0 6 3 5 1 9 6 9 9\n 7 8 5 1 0 5 8 3 5 2 4 4 2 7 9 3 6 1 4 8 3 3 2 0 2 7 0 1 4 2 2 6 1 4 6 6 3\n 9 0 7 0 9 0 8 1 2 9 1 8 6 9 7 5 7 5 2 6 9 3 2 4 5 5 4 8 4 9 4 3 7 8 9 2 9\n 4 4 1 9 1 7 4 8 8 5 7 9 6 7 2 6 2 6 0 3 8 7 2 5 0 3 3 8 2 2 1 9 0 5 4 3 1\n 6 1 8 4 1 9 8 2 9 0 6 4 5 0 3 1 8 9 8 7 8 7 4 6 6 7 4 0 1 8 6 2 4 8 4 1 8\n 6 0 7 4 1 4 9 4 4 1 3 0 1 6 0 3 4 0 9 2 3 9 3 5 4 2 3 0 7 6 6 0 9 2 5 7 5\n 1 8 7 6 3 1 4 6 8 7 6 1 7 0 7 3 3 0 1 1 0 4 1 5 1 6 6 2 2 2 6 2 7 4 0 9 2\n 5 8 5 1 0 4 0 0 7 5 3 0 0 8 3 9 7 1 5 4 7 0 2 2 4 7 8 8 6 9 9 8 2 0 0 3 2\n 3 5 0 0 9 3 1 5 2 6 5 4 5 4 6 3 0 5 5 6 0 9 2 8 2 9 8 8 3 9 3 1 0 4 0 7 6\n 5 1 6 6 2 5 1 9 1 9 9 1 3 3 4 2 8 2 0 1 1 0 0 8 6 3 5 1 4 6 0 7 2 2 4 3 6\n 3 0 6 9 6 3 5 3 4 2 1 5 0 0 6 9 6 0 0 3 6 4 3 6 5 2 2 7 0 5 6 8 1 4 4 8 3\n 7 9 5 5 0 6 8 2 6 3 2 9 6 1 2 4 7 0 7 9 3 1 9 0 9 3 9 1 9 0 6 6 1 5 5 4 7\n 1 3 6 8 2 7 3 3 7 8 8 1 4 2 0 4 8 4 0 5 3 0 7 2 6 5 2 7 2 8 5 9 0 1 9 6 5\n 2 6 8 9 3 1 6 7 6 8 2 1 1 7 3 2 6 3 6 9 9 8 5 9 6 1 6 4 7 8 9 3 5 1 5 9 1\n 4 4 2 0 0 9 0 4 2 8 4 7 2 6 6 7 8 7 7 9 9 6 5 7 2 2 2 1 3 4 6 5 9 3 3 6 0\n 8 0 9 0 7 2 8 2 4 5 4 1 4 6 7 9 5 0 0 7 0 1 1 1 8 2 4 4 6 9 4 0 4 9 4 3 0\n 0 5 4 2 1 4 0 6 8 0 0 1 5 8 7 9 5 4 1 9 2 7 1 6 1 8 7 6 9 7 0 2 5 4 0 4 9\n 7 1 1 8 3 9 9 4 8 4 4 0 0 0 7 8 2 2 4 7 0 0 1 2 2 6 3 8 4 4 4 9 5 0 4 9 0\n 1 3 4 6 3 5 2 5 9 7 2 7 1 1 5 7 9 0 1 9 6 4 5 8 2 0 0 3 2 7 1 5 9 6 3 3 8\n 5 4 2 7 5 2 6 0 6 3 3 8 1 1 7 0 7 6 1 8 7 6 5 3 7 8 5 8 5 5 3 7 1 2 0 8 2\n 0 5 8 7 1 9 1 7 9 9 1 9 2 6 6 1 6 8 9 9 3 8 6 9 0 1 0 5 0 0 9 4 3 5 6 2 3\n 5 3 5 0 2 5 3 7 5 1 7 0 8 1 4 8 9 6 5 8 3 6 7 7 2 5 0 7 8 8 2 4 4 2 0 2 6\n 3 2 0 2 1 9 7 2 5 7 3 6 4 0 9 4 2 3 7 4 9 6 6 9 7 1 8 2 6 1 9 5 9 3 2 8 2\n 3] [9 6 4 9 1 7 9 9 5 3 9 3 9 9 5 2 4 4 1 1 1 7 4 2 6 4 1 2 6 3 5 2 3 9 8 2 3\n 9 1 2 9 7 4 6 5 4 7 0 6 5 2 0 6 6 9 2 8 4 2 9 3 6 9 2 4 5 3 9 7 9 6 1 8 4\n 3 6 4 7 5 4 5 2 7 3 3 2 2 2 6 8 8 4 2 5 9 7 4 9 2 8 6 7 2 1 4 7 0 3 5 2 8\n 0 1 0 9 0 1 3 3 5 4 0 8 5 8 8 5 0 0 7 6 9 8 9 7 1 5 2 5 5 0 0 7 7 3 0 4 9\n 3 0 7 8 6 1 6 9 7 0 6 0 3 3 6 3 4 1 2 6 7 5 0 0 4 2 0 9 0 6 3 0 1 9 6 9 9\n 7 8 8 1 0 5 5 3 5 2 4 4 2 7 9 3 6 1 4 8 7 8 2 0 2 7 0 3 4 2 2 6 7 4 6 6 3\n 9 0 7 5 9 0 5 1 2 9 1 8 6 9 7 5 7 9 2 6 9 3 2 4 5 5 4 8 4 9 4 3 7 8 9 2 4\n 4 4 1 9 1 9 4 8 0 5 7 9 6 1 2 6 2 6 0 3 8 9 2 5 5 3 3 1 2 2 2 9 0 5 4 3 2\n 6 1 0 4 1 4 8 3 9 0 6 4 5 0 3 1 8 9 8 7 1 7 4 6 6 7 4 0 3 3 4 2 4 2 4 1 8\n 6 0 9 4 1 4 9 4 4 1 3 5 1 6 0 3 4 0 9 2 5 9 3 0 4 2 3 0 7 6 6 6 9 2 3 7 5\n 1 8 7 6 3 1 4 6 8 7 6 2 7 0 7 3 3 0 1 1 0 4 1 5 1 6 6 8 2 2 6 2 7 4 0 9 2\n 3 8 5 1 0 4 0 0 2 5 3 0 0 8 3 9 7 1 5 4 7 0 2 2 4 9 5 8 6 9 9 8 7 0 0 3 8\n 0 3 0 0 9 3 1 5 2 6 5 4 0 4 6 5 0 5 5 6 0 9 6 8 2 9 8 8 3 9 3 2 0 4 0 7 5\n 5 1 4 6 2 5 1 9 1 9 9 1 3 3 4 2 8 2 0 1 1 0 2 8 6 3 0 1 4 6 0 7 8 2 4 3 6\n 3 0 6 9 6 3 5 3 4 2 1 3 2 0 4 9 6 0 0 3 6 4 3 6 5 2 2 9 0 6 6 8 1 9 4 8 3\n 7 9 0 9 0 6 8 2 6 3 2 9 6 1 2 4 7 0 7 9 3 2 9 0 9 3 9 1 4 0 6 6 1 5 5 4 7\n 1 3 6 8 2 7 3 9 9 8 8 1 4 2 0 4 8 4 5 5 3 6 2 2 6 5 2 7 0 9 5 7 0 1 9 6 5\n 2 6 8 9 3 1 6 7 6 8 2 1 1 7 3 2 6 3 6 9 9 8 5 9 6 1 6 4 7 8 7 3 5 1 5 9 1\n 4 9 4 0 0 4 0 4 2 8 4 7 2 6 6 7 8 7 7 9 9 6 5 7 2 2 2 1 5 4 6 5 9 8 3 6 0\n 1 0 9 2 7 3 8 2 4 5 4 1 4 6 7 9 5 0 0 7 0 1 1 1 8 2 9 4 6 9 4 0 4 9 4 3 2\n 0 5 4 7 2 4 0 6 8 0 0 1 5 8 7 9 5 4 1 9 2 4 1 6 1 5 7 6 9 7 0 2 5 4 0 4 9\n 7 1 1 8 3 4 9 4 8 4 4 0 0 5 0 8 2 2 9 7 0 0 1 2 2 6 3 8 4 4 4 9 4 0 4 9 2\n 1 1 4 6 3 5 8 5 9 7 3 7 1 1 5 7 9 0 1 9 6 4 5 5 2 0 0 3 3 7 8 5 9 6 3 3 8\n 5 4 2 7 5 2 5 0 6 3 3 8 1 1 7 0 7 5 1 8 7 6 5 3 7 5 6 8 5 5 5 7 1 2 0 8 2\n 0 5 8 7 1 9 1 7 8 9 1 9 3 6 6 1 6 5 9 9 3 8 6 8 0 1 0 8 0 0 9 6 3 2 6 2 3\n 5 3 5 0 2 5 8 7 5 1 7 2 8 2 2 8 9 6 5 8 3 6 7 7 2 8 0 7 8 8 2 5 4 2 0 2 6\n 3 2 0 2 1 9 7 2 5 7 2 6 4 0 9 4 3 3 7 4 9 6 6 9 7 2 4 2 6 1 9 0 9 3 2 8 2\n 5]\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"0.864"},"metadata":{}}]}]}